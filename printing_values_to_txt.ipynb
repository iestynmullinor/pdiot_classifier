{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GYRO = False\n",
    "SEQUENCE_LENGTH = 5\n",
    "SEQUENCE_OVERLAP = 3\n",
    "BATCH_SIZE = 5\n",
    "EPOCHS = 20\n",
    "CNN = True\n",
    "MODEL_NAME = f\"AllInOne_CNN:{CNN}_pochs:{EPOCHS}_batch:{BATCH_SIZE}_gyro:{GYRO}_window:{SEQUENCE_LENGTH}_overlap:{SEQUENCE_OVERLAP}\"\n",
    "DEV_SIZE = 0\n",
    "TEST_SIZE = 0\n",
    "LEAVE_ONE_OUT = False\n",
    "NORMALISE = True\n",
    "OVERLAP_ON_TEST_SET = False\n",
    "TRANSFORM = True\n",
    "AUGMENT = False\n",
    "AUGMENT_SIZE = 2 # 1 means 1 extra augmented sample per sample, 2 means 2 extra augmented samples per sample etc.\n",
    "AUGMENT_NOISE = 0.03\n",
    "DIFFERENTIALS = True\n",
    "GRADIENTS = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code takes in a position, eg \"sitting_or_standing\", \"lying_down_left\" etc and trains a model for just classifying activities of that position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.file_tagger as file_tagger\n",
    "import helpers.sequence_generator as sequence_generator\n",
    "import helpers.split_by_student as split_by_student\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers, Sequential, models, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n",
    "POSSIBLE_ACTIVITIES = [\n",
    "    \"sitting&coughing\",\n",
    "    \"sitting&hyperventilating\",\n",
    "    \"sitting&normal_breathing\",\n",
    "    \n",
    "    \"standing&coughing\",\n",
    "    \"standing&hyperventilating\",\n",
    "    \"standing&normal_breathing\",\n",
    "    \n",
    "    \"lying_down_left&normal_breathing\",\n",
    "    \"lying_down_left&coughing\",\n",
    "    \"lying_down_left&hyperventilating\",\n",
    "\n",
    "    \"lying_down_right&normal_breathing\",\n",
    "    \"lying_down_right&coughing\",\n",
    "    \"lying_down_right&hyperventilating\",\n",
    "\n",
    "    \"lying_down_back&normal_breathing\",\n",
    "    \"lying_down_back&coughing\",\n",
    "    \"lying_down_back&hyperventilating\",\n",
    "\n",
    "    \"lying_down_stomach&normal_breathing\",\n",
    "    \"lying_down_stomach&coughing\",\n",
    "    \"lying_down_stomach&hyperventilating\",\n",
    "\n",
    "]\n",
    "  \n",
    "\n",
    "POSSIBLE_OUTCOMES= [\n",
    "    \"sitting_or_standing&normal_breathing\",\n",
    "    \"sitting_or_standing&coughing\",\n",
    "    \"sitting_or_standing&hyperventilating\",\n",
    "\n",
    "    \"lying_down_left&normal_breathing\",\n",
    "    \"lying_down_left&coughing\",\n",
    "    \"lying_down_left&hyperventilating\",\n",
    "\n",
    "    \"lying_down_right&normal_breathing\",\n",
    "    \"lying_down_right&coughing\",\n",
    "    \"lying_down_right&hyperventilating\",\n",
    "\n",
    "    \"lying_down_back&normal_breathing\",\n",
    "    \"lying_down_back&coughing\",\n",
    "    \"lying_down_back&hyperventilating\",\n",
    "\n",
    "    \"lying_down_stomach&normal_breathing\",\n",
    "    \"lying_down_stomach&coughing\",\n",
    "    \"lying_down_stomach&hyperventilating\",    \n",
    "]\n",
    "\n",
    "DATA_DIRECTORY = \"./all_respeck\"\n",
    "LABEL_TO_INDEX = {label: idx for idx, label in enumerate(POSSIBLE_OUTCOMES)}\n",
    "\n",
    "if OVERLAP_ON_TEST_SET:\n",
    "    TEST_SEQUENCE_OVERLAP = SEQUENCE_OVERLAP\n",
    "else:\n",
    "    TEST_SEQUENCE_OVERLAP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(directory, sequence_length, overlap, file_names, gyro = GYRO): # if gyro is false, only accelerometer data is used\n",
    "\n",
    "    tagged_data = []\n",
    "\n",
    "    # group each csv file into their respective areas\n",
    "    csv_dictionary = file_tagger.tag_directory(directory)\n",
    "\n",
    "    # iterates through each activity\n",
    "    for key in POSSIBLE_ACTIVITIES:\n",
    "\n",
    "        # iterates through each csv file for the activity \n",
    "        for csv_file in csv_dictionary[key]:\n",
    "            if csv_file in file_names:\n",
    "                if gyro:\n",
    "                    sequences = sequence_generator.generate_sequences_from_file_with_gyroscope(directory + \"/\" + csv_file, sequence_length, overlap, normalise=NORMALISE)\n",
    "                else:\n",
    "                    sequences = sequence_generator.generate_sequences_from_file_without_gyroscope(directory + \"/\" + csv_file, sequence_length, overlap, normalise=NORMALISE)\n",
    "\n",
    "                # iterate through each generated sequence\n",
    "                for sequence in sequences:\n",
    "                    position = key.split(\"&\")[0]\n",
    "                    activity = key.split(\"&\")[1]\n",
    "\n",
    "                    if activity == \"talking\" or activity == \"singing\" or activity == \"laughing\" or activity == \"eating\":\n",
    "                        activity = \"other\"\n",
    "\n",
    "                    if position == \"standing\" or position == \"sitting\":\n",
    "                        position = \"sitting_or_standing\"\n",
    "                        \n",
    "                    tagged_data.append((position + \"&\" + activity, sequence))\n",
    "\n",
    "    print (\"there are \" + str(len(tagged_data)) + \" tagged sequences in the dataset\")\n",
    "\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_CNN(input_data, labels_encoded, unique_labels, epochs, batch_size, validation_data):\n",
    "    print(\"Training CNN model\")\n",
    "    if GYRO:\n",
    "        width = 6\n",
    "    else:\n",
    "        width = 3\n",
    "    if TRANSFORM:\n",
    "        width = width+3\n",
    "    if DIFFERENTIALS:\n",
    "        width = width+3\n",
    "    if GRADIENTS:\n",
    "        width = width+3\n",
    "    # Define the CNN model for your specific input shape\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(32, 3, activation='relu', input_shape=(SEQUENCE_LENGTH*25, width)),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(128, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        #layers.Conv1D(256, 3, activation='relu'),\n",
    "        #layers.MaxPooling1D(2),\n",
    "        #layers.Conv1D(512, 3, activation='relu'),\n",
    "        #layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.6),\n",
    "        layers.Flatten(),\n",
    "        #layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu', activity_regularizer=regularizers.l2(0.05)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(len(unique_labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the CNN model\n",
    "    if len(validation_data[0]) == 0:\n",
    "        model.fit(input_data, labels_encoded, epochs=epochs, batch_size=batch_size)\n",
    "    else:\n",
    "        model.fit(input_data, labels_encoded, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_LSTM(input_data, labels_encoded, unique_labels, epochs, batch_size, validation_data):\n",
    "    print(\"\\n\\n ===== Training LSTM Model =====\\n\")\n",
    "    if GYRO:\n",
    "        width = 6\n",
    "    else:\n",
    "        width = 3\n",
    "    if TRANSFORM:\n",
    "        width = width+3\n",
    "    if DIFFERENTIALS:\n",
    "        width = width+3\n",
    "    model = Sequential([\n",
    "        \n",
    "        layers.LSTM(64, activation='relu', return_sequences=True, input_shape=(SEQUENCE_LENGTH*25, width)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.LSTM(64, return_sequences=True, activation='relu'),\n",
    "        layers.LSTM(128, return_sequences=True, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(len(unique_labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the CNN model\n",
    "    if len(validation_data[0]) == 0:\n",
    "        model.fit(input_data, labels_encoded, epochs=epochs, batch_size=batch_size)\n",
    "    else:\n",
    "        model.fit(input_data, labels_encoded, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(sequences, labels, num_augmentations):\n",
    "    augmented_sequences = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for sequence, label in zip(sequences, labels):\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_sequence = sequence + np.random.normal(0, AUGMENT_SIZE, size=sequence.shape)\n",
    "            augmented_sequences.append(augmented_sequence)\n",
    "            augmented_labels.append(label)\n",
    "    \n",
    "    return augmented_sequences, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft(data):\n",
    "\n",
    "    # Extract x, y, and z data\n",
    "    x_data = data[:, 0]\n",
    "    y_data = data[:, 1]\n",
    "    z_data = data[:, 2]\n",
    "\n",
    "    # Apply FFT to each axis\n",
    "    x_fft = np.fft.fft(x_data)\n",
    "    y_fft = np.fft.fft(y_data)\n",
    "    z_fft = np.fft.fft(z_data)\n",
    "\n",
    "    # The result is complex numbers, so you may want to take the magnitude\n",
    "    x_magnitude = np.abs(x_fft)\n",
    "    y_magnitude = np.abs(y_fft)\n",
    "    z_magnitude = np.abs(z_fft)\n",
    "\n",
    "    # If needed, you can also compute the corresponding frequencies\n",
    "    # The frequencies are in cycles per time unit (usually, Hz if your time unit is seconds)\n",
    "    x_frequencies = np.fft.fftfreq(len(x_data))\n",
    "    y_frequencies = np.fft.fftfreq(len(y_data))\n",
    "    z_frequencies = np.fft.fftfreq(len(z_data))\n",
    "\n",
    "    representation = []\n",
    "    for i in range(len(x_magnitude)):\n",
    "        representation.append([x_magnitude[i], y_magnitude[i], z_magnitude[i]]) #, x_frequencies[i], y_frequencies[i], z_frequencies[i]])\n",
    "\n",
    "    return representation\n",
    "\n",
    "def extract_fft(train_data, dev_data, test_data):\n",
    "    train_features = [fft(sequence) for sequence in train_data]\n",
    "    dev_features = [fft(sequence) for sequence in dev_data]\n",
    "    test_features = [fft(sequence) for sequence in test_data]\n",
    "\n",
    "    return train_features, dev_features, test_features\n",
    "\n",
    "\n",
    "def merge_arrays(arr1, arr2):\n",
    "    return np.concatenate((arr1, arr2), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalise(sequence):\n",
    "    \"\"\"\n",
    "    Normalizes a matrix of accelerometer values.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(sequence, axis=1)\n",
    "    norm[norm == 0] = 1\n",
    "    return sequence / norm[:, np.newaxis]\n",
    "\n",
    "def normalise_data(data):\n",
    "    \"\"\"\n",
    "    Normalizes a list of matrices of accelerometer values.\n",
    "    \"\"\"\n",
    "    return [normalise(sequence) for sequence in data]\n",
    "\n",
    "def differential(data):\n",
    "    # Extract x, y, and z data\n",
    "    x_data = data[:, 0]\n",
    "    y_data = data[:, 1]\n",
    "    z_data = data[:, 2]\n",
    "\n",
    "    # Compute the differences between consecutive data points\n",
    "    x_diff = np.diff(x_data)\n",
    "    y_diff = np.diff(y_data)\n",
    "    z_diff = np.diff(z_data)\n",
    "\n",
    "    # Pad the differential variables to match the length of the data variables\n",
    "    x_diff = np.pad(x_diff, (0, 1), mode='constant')\n",
    "    y_diff = np.pad(y_diff, (0, 1), mode='constant')\n",
    "    z_diff = np.pad(z_diff, (0, 1), mode='constant')\n",
    "\n",
    "    # Combine the differential values into a representation\n",
    "    representation = []\n",
    "    for i in range(len(x_diff)):\n",
    "        representation.append([x_diff[i], y_diff[i], z_diff[i]])\n",
    "\n",
    "    return representation\n",
    "\n",
    "def extract_differentials(train_data, dev_data, test_data):\n",
    "    train_features = [differential(sequence) for sequence in train_data]\n",
    "    \n",
    "    dev_features = [differential(sequence) for sequence in dev_data]\n",
    "    \n",
    "    test_features = [differential(sequence) for sequence in test_data]\n",
    "\n",
    "    return train_features, dev_features, test_features\n",
    "\n",
    "def derivative(data):\n",
    "    # Extract x, y, and z data\n",
    "    x_data = data[:, 0]\n",
    "    y_data = data[:, 1]\n",
    "    z_data = data[:, 2]\n",
    "\n",
    "    # Compute the derivative of the data\n",
    "    x_derivative = np.gradient(x_data)\n",
    "    y_derivative = np.gradient(y_data)\n",
    "    z_derivative = np.gradient(z_data)\n",
    "\n",
    "    # Combine the derivative values into a representation\n",
    "    representation = []\n",
    "    for i in range(len(x_derivative)):\n",
    "        representation.append([x_derivative[i], y_derivative[i], z_derivative[i]])\n",
    "\n",
    "    return representation\n",
    "\n",
    "def extract_gradients(train_data, dev_data, test_data):\n",
    "    train_features = [derivative(sequence) for sequence in train_data]\n",
    "    dev_features = [derivative(sequence) for sequence in dev_data]\n",
    "    test_features = [derivative(sequence) for sequence in test_data]\n",
    "\n",
    "    return train_features, dev_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_label_lists(tagged_sequences):\n",
    "    sequences = [sequence for _, sequence in tagged_sequences]\n",
    "    labels = [label for label, _ in tagged_sequences]\n",
    "    sequences = np.array(sequences, dtype=np.float32)\n",
    "    labels_encoded = [LABEL_TO_INDEX[label] for label in labels]\n",
    "    labels = np.array(labels_encoded)\n",
    "\n",
    "    return sequences, labels\n",
    "    \n",
    "\n",
    "def create_data_sets(dev_size, test_size):\n",
    "\n",
    "    training_files, dev_files, test_files = split_by_student.split_data(students_in_dev_set= dev_size, students_in_test_set=test_size)\n",
    "\n",
    "    tagged_training_sequences = generate_training_data(DATA_DIRECTORY, SEQUENCE_LENGTH, SEQUENCE_OVERLAP, file_names=training_files)\n",
    "    tagged_dev_sequences = generate_training_data(DATA_DIRECTORY, SEQUENCE_LENGTH, TEST_SEQUENCE_OVERLAP, file_names=dev_files)\n",
    "    tagged_test_sequences = generate_training_data(DATA_DIRECTORY, SEQUENCE_LENGTH, TEST_SEQUENCE_OVERLAP, file_names=test_files)\n",
    "\n",
    "    train_data, train_labels = create_sequence_label_lists(tagged_training_sequences)\n",
    "    dev_data, dev_labels = create_sequence_label_lists(tagged_dev_sequences)\n",
    "    test_data, test_labels = create_sequence_label_lists(tagged_test_sequences)\n",
    "\n",
    "    #print(len(train_data), len(train_labels), len(dev_data), len(dev_labels), len(test_data), len(test_labels))\n",
    "\n",
    "    return train_data, train_labels, dev_data, dev_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out(isCNN=CNN):\n",
    "    students = split_by_student.get_list_of_stutents()\n",
    "    test_accuracies = []\n",
    "    for test_student in students:\n",
    "        print(\"testing student: \" + str(test_student))\n",
    "        print(\"training students: \" + str([student for student in students if student != test_student]))\n",
    "        \n",
    "        test_files, training_files = split_by_student.get_list_of_files(test_student)\n",
    "\n",
    "        tagged_training_sequences = generate_training_data(DATA_DIRECTORY, SEQUENCE_LENGTH, SEQUENCE_OVERLAP, file_names=training_files)\n",
    "        tagged_test_sequences = generate_training_data(DATA_DIRECTORY, SEQUENCE_LENGTH, TEST_SEQUENCE_OVERLAP, file_names=test_files)\n",
    "\n",
    "        train_data, train_labels = create_sequence_label_lists(tagged_training_sequences)\n",
    "        test_data, test_labels = create_sequence_label_lists(tagged_test_sequences)\n",
    "\n",
    "        RAW_TRAIN = train_data[0]\n",
    "\n",
    "        if AUGMENT:\n",
    "            augmented_train_data, augmented_train_labels = augment_data(train_data, train_labels, AUGMENT_SIZE)\n",
    "            train_data = np.concatenate((train_data, augmented_train_data))\n",
    "            train_labels = np.concatenate((train_labels, augmented_train_labels))\n",
    "            print(\"length of training data with augmentation: \" + str(len(train_data)))\n",
    "\n",
    "        if TRANSFORM:\n",
    "            train_transform, dev_transform, test_transform = extract_fft(train_data, None, test_data)\n",
    "            train_data = np.array(train_data)\n",
    "            train_transform = np.array(train_transform)\n",
    "           \n",
    "            test_data = np.array(test_data)\n",
    "            test_transform = np.array(test_transform)\n",
    "\n",
    "            train_data = np.array([merge_arrays(train_data[i], train_transform[i]) for i in range(len(train_data))])\n",
    "            test_data = np.array([merge_arrays(test_data[i], test_transform[i]) for i in range(len(test_data))])\n",
    "\n",
    "        if DIFFERENTIALS:\n",
    "            train_differential, dev_differential, test_differential = extract_differentials(train_data, None, test_data)\n",
    "            train_data = np.array(train_data)\n",
    "            train_differential = np.array(train_differential)\n",
    "           \n",
    "            test_data = np.array(test_data)\n",
    "            test_differential = np.array(test_differential)\n",
    "\n",
    "            train_data = np.array([merge_arrays(train_data[i], train_differential[i]) for i in range(len(train_data))])\n",
    "            test_data = np.array([merge_arrays(test_data[i], test_differential[i]) for i in range(len(test_data))])\n",
    "\n",
    "        \n",
    "        if GRADIENTS:\n",
    "            train_derivatives, dev_derivatives, test_derivatives = extract_gradients(train_data, dev_data, test_data)\n",
    "            train_data = np.array(train_data)\n",
    "            train_derivatives = np.array(train_derivatives)\n",
    "            dev_data = np.array(dev_data)\n",
    "            dev_derivatives = np.array(dev_derivatives)\n",
    "            test_data = np.array(test_data)\n",
    "            test_derivatives = np.array(test_derivatives)\n",
    "\n",
    "            train_data = np.array([merge_arrays(train_data[i], train_derivatives[i]) for i in range(len(train_data))])\n",
    "            dev_data = np.array([merge_arrays(dev_data[i], dev_derivatives[i]) for i in range(len(dev_data))])\n",
    "            test_data = np.array([merge_arrays(test_data[i], test_derivatives[i]) for i in range(len(test_data))])\n",
    "\n",
    "\n",
    "        print(train_data)\n",
    "\n",
    "        if isCNN:\n",
    "            model = train_model_CNN(train_data, train_labels, POSSIBLE_OUTCOMES, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([], []))\n",
    "        else:\n",
    "            model = train_model_LSTM(train_data, train_labels, POSSIBLE_OUTCOMES, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([], []))\n",
    "        \n",
    "        test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(\"for student \" + str(test_student) + \" the accuracy is \" + str(test_accuracy))\n",
    "        print(\"average accuracy so far: \" + str(sum(test_accuracies)/len(test_accuracies)))\n",
    "        print(\"number of students tested so far: \" + str(len(test_accuracies)))\n",
    "        time.sleep(3)\n",
    "\n",
    "        \n",
    "    print(\"Accuracy for each student:\")\n",
    "    print(\", \".join([f\"{student}: {accuracy}\" for student, accuracy in zip(students, test_accuracies)]))\n",
    "    print(\"Average overall accuracy:\", sum(test_accuracies)/len(test_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: s83, s15, s84, s13, s56, s82, s59, s30, s33, s80, s51, s72, s91, s16, s36, s40, s71, s93, s96, s39, s35, s34, s48, s5, s97, s98, s44, s70, s92, s9, s63, s64, s7, s52, s60, s100, s23, s38, s67, s66, s8, s42, s18, s12, s3, s11, s95, s77, s50, s65, s32, s21, s102, s27, s57, s79, s17, s54, s75, s87, s61, s43, s86, s74, s29, s88, s45, s46, s55, s22, s1\n",
      "Dev Set: \n",
      "Test Set: \n",
      "there are 16452 tagged sequences in the dataset\n",
      "there are 0 tagged sequences in the dataset\n",
      "there are 0 tagged sequences in the dataset\n",
      "[[-0.49682617 -0.8357544   0.34490967]\n",
      " [-0.38720703 -1.159729    0.21282959]\n",
      " [-0.42993164 -0.86798096  0.22015381]\n",
      " [-0.43701172 -0.70977783  0.28045654]\n",
      " [-0.3737793  -0.93951416  0.23138428]\n",
      " [-0.29467773 -0.9727173   0.25872803]\n",
      " [-0.45776367 -1.1372681   0.199646  ]\n",
      " [-0.3708496  -1.0569458   0.16400146]\n",
      " [-0.41601562 -0.81988525  0.26141357]\n",
      " [-0.45581055 -0.82696533  0.16107178]\n",
      " [-0.34814453 -0.88409424  0.24945068]\n",
      " [-0.35742188 -1.0413208   0.18450928]\n",
      " [-0.3737793  -0.9924927   0.18133545]\n",
      " [-0.29858398 -0.7979126   0.18011475]\n",
      " [-0.39941406 -0.7815552   0.24285889]\n",
      " [-0.47216797 -0.78692627  0.29632568]\n",
      " [-0.35498047 -0.9216919   0.12713623]\n",
      " [-0.37036133 -1.0252075   0.28729248]\n",
      " [-0.39501953 -1.0183716   0.14935303]\n",
      " [-0.32104492 -0.8845825   0.18939209]\n",
      " [-0.33496094 -0.86798096  0.15423584]\n",
      " [-0.48388672 -1.093811    0.16912842]\n",
      " [-0.3305664  -1.1038208   0.2921753 ]\n",
      " [-0.51342773 -0.6856079   0.21575928]\n",
      " [-0.3408203  -0.57598877  0.21429443]\n",
      " [-0.47607422 -1.0776978   0.3024292 ]\n",
      " [-0.3557129  -1.1372681   0.16229248]\n",
      " [-0.34106445 -0.87823486  0.18450928]\n",
      " [-0.32006836 -0.76885986  0.17962646]\n",
      " [-0.43554688 -0.88287354  0.28045654]\n",
      " [-0.37695312 -1.0479126   0.21502686]\n",
      " [-0.34106445 -0.89434814  0.19476318]\n",
      " [-0.37890625 -0.8118286   0.24359131]\n",
      " [-0.2939453  -0.9043579   0.24212646]\n",
      " [-0.3725586  -0.95440674  0.2399292 ]\n",
      " [-0.39697266 -0.8809204   0.26507568]\n",
      " [-0.37695312 -0.8640747   0.22406006]\n",
      " [-0.38867188 -0.9055786   0.26971436]\n",
      " [-0.31982422 -0.9458618   0.27166748]\n",
      " [-0.37524414 -0.9246216   0.2506714 ]\n",
      " [-0.3894043  -0.92926025  0.25872803]\n",
      " [-0.4333496  -0.85357666  0.11199951]\n",
      " [-0.390625   -0.9932251   0.38690186]\n",
      " [-0.5202637  -0.91241455  0.26849365]\n",
      " [-0.40771484 -0.90802     0.35882568]\n",
      " [-0.42089844 -1.000061    0.22747803]\n",
      " [-0.53686523 -0.8723755   0.24481201]\n",
      " [-0.34570312 -0.77423096  0.28729248]\n",
      " [-0.35717773 -0.9373169   0.16790771]\n",
      " [-0.40942383 -0.9846802   0.20574951]\n",
      " [-0.36206055 -1.0098267   0.22161865]\n",
      " [-0.34204102 -0.815979    0.12322998]\n",
      " [-0.2709961  -0.84747314  0.21331787]\n",
      " [-0.4074707  -0.94537354  0.15814209]\n",
      " [-0.3544922  -0.8328247   0.02581787]\n",
      " [-0.4033203  -0.91412354  0.3637085 ]\n",
      " [-0.50146484 -0.8164673   0.18548584]\n",
      " [-0.41064453 -1.0293579   0.3241577 ]\n",
      " [-0.48583984 -1.0300903   0.2694702 ]\n",
      " [-0.5161133  -0.82940674  0.31610107]\n",
      " [-0.35986328 -0.7710571   0.23236084]\n",
      " [-0.41064453 -0.8928833   0.1593628 ]\n",
      " [-0.37573242 -1.0108032   0.2164917 ]\n",
      " [-0.32983398 -0.9956665   0.26385498]\n",
      " [-0.3564453  -0.87579346  0.1906128 ]\n",
      " [-0.38500977 -0.86016846  0.15887451]\n",
      " [-0.33154297 -0.91192627  0.2658081 ]\n",
      " [-0.40698242 -0.91412354  0.16815186]\n",
      " [-0.34423828 -0.9146118   0.21185303]\n",
      " [-0.33129883 -0.93414307  0.2765503 ]\n",
      " [-0.42236328 -0.91851807  0.27581787]\n",
      " [-0.40966797 -0.923645    0.24432373]\n",
      " [-0.36938477 -0.8640747   0.22137451]\n",
      " [-0.30737305 -0.8604126   0.22845459]\n",
      " [-0.34985352 -0.9036255   0.23480225]\n",
      " [-0.4091797  -0.9343872   0.26922607]\n",
      " [-0.39038086 -0.9295044   0.22625732]\n",
      " [-0.3503418  -0.88116455  0.23480225]\n",
      " [-0.36645508 -0.9126587   0.23553467]\n",
      " [-0.3779297  -0.95977783  0.2230835 ]\n",
      " [-0.3564453  -0.973938    0.230896  ]\n",
      " [-0.28271484 -0.6502075   0.36419678]\n",
      " [-0.41601562 -0.82940674  0.27703857]\n",
      " [-0.36791992 -1.0874634   0.22064209]\n",
      " [-0.41308594 -0.93927     0.32196045]\n",
      " [-0.5197754  -0.70758057  0.21063232]\n",
      " [-0.4111328  -0.9404907   0.2802124 ]\n",
      " [-0.34350586 -1.0410767   0.3048706 ]\n",
      " [-0.42944336 -0.84869385  0.11981201]\n",
      " [-0.33666992 -0.90802     0.21673584]\n",
      " [-0.31274414 -0.9661255   0.14300537]\n",
      " [-0.36254883 -0.89849854  0.14886475]\n",
      " [-0.1315918  -0.8032837  -0.02935791]\n",
      " [-0.30566406 -0.7854614   0.25653076]\n",
      " [-0.38134766 -0.8826294   0.09832764]\n",
      " [-0.3894043  -1.1956177   0.2518921 ]\n",
      " [-0.4309082  -0.98443604  0.3517456 ]\n",
      " [-0.57958984 -0.75201416  0.19085693]\n",
      " [-0.39331055 -0.864563    0.29974365]\n",
      " [-0.35498047 -0.9697876   0.23919678]\n",
      " [-0.39086914 -0.84210205  0.1786499 ]\n",
      " [-0.3557129  -0.97662354  0.2255249 ]\n",
      " [-0.328125   -0.9026489   0.11126709]\n",
      " [-0.34545898 -0.91192627  0.16400146]\n",
      " [-0.39648438 -0.92633057  0.21746826]\n",
      " [-0.43603516 -0.87506104  0.23065186]\n",
      " [-0.48046875 -0.8340454   0.05072021]\n",
      " [-0.49243164 -1.0437622   0.6911011 ]\n",
      " [-0.3696289  -0.6494751   0.24017334]\n",
      " [-0.44677734 -1.055481    0.496521  ]\n",
      " [-0.5739746  -1.0740356   0.24676514]\n",
      " [-0.4868164  -0.7866821   0.2904663 ]\n",
      " [-0.34521484 -0.73809814  0.2538452 ]\n",
      " [-0.36865234 -0.8501587   0.19378662]\n",
      " [-0.45507812 -1.0667114   0.27972412]\n",
      " [-0.36547852 -0.954895    0.24139404]\n",
      " [-0.36791992 -0.7979126   0.15765381]\n",
      " [-0.36572266 -0.87872314  0.22113037]\n",
      " [-0.4248047  -0.9451294   0.27044678]\n",
      " [-0.46020508 -0.9404907   0.26409912]\n",
      " [-0.39794922 -0.91192627  0.20379639]\n",
      " [-0.3737793  -0.85772705  0.25335693]\n",
      " [-0.42163086 -0.87701416  0.22503662]\n",
      " [-0.32666016 -0.94195557  0.2736206 ]\n",
      " [-0.3852539  -0.9031372   0.2033081 ]]\n",
      "[[-0.49682617 -0.83575439  0.34490967 ...  0.10961914 -0.32397461\n",
      "  -0.13208008]\n",
      " [-0.38720703 -1.159729    0.21282959 ...  0.03344727 -0.01611328\n",
      "  -0.06237793]\n",
      " [-0.42993164 -0.86798096  0.22015381 ... -0.02490234  0.22497559\n",
      "   0.03381348]\n",
      " ...\n",
      " [-0.42163086 -0.87701416  0.22503662 ...  0.02355957 -0.04211426\n",
      "   0.01013184]\n",
      " [-0.32666016 -0.94195557  0.27362061 ...  0.01818848 -0.01306152\n",
      "  -0.01086426]\n",
      " [-0.38525391 -0.90313721  0.20330811 ... -0.05859375  0.03881836\n",
      "  -0.0703125 ]]\n",
      "Training CNN model\n",
      "Epoch 1/20\n",
      "1046/3291 [========>.....................] - ETA: 6s - loss: 1.2205 - accuracy: 0.6470"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m row) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m},\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m CNN:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     model \u001b[39m=\u001b[39m train_model_CNN(train_data, train_labels, POSSIBLE_OUTCOMES, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, epochs\u001b[39m=\u001b[39;49mEPOCHS, validation_data\u001b[39m=\u001b[39;49m(dev_data, dev_labels)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     model \u001b[39m=\u001b[39m train_model_LSTM(train_data, train_labels, POSSIBLE_OUTCOMES, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, epochs\u001b[39m=\u001b[39mEPOCHS, validation_data\u001b[39m=\u001b[39m(dev_data, dev_labels)) \n",
      "\u001b[1;32m/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Train the CNN model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(validation_data[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(input_data, labels_encoded, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/iestyn/pdiot/model_trials/pdiot_classifier/task2_allin1.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     model\u001b[39m.\u001b[39mfit(input_data, labels_encoded, epochs\u001b[39m=\u001b[39mepochs, batch_size\u001b[39m=\u001b[39mbatch_size, validation_data\u001b[39m=\u001b[39mvalidation_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:250\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m InterpolateRuntimeError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    244\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcontrol_dependencies(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_options\u001b[39m.\u001b[39mcontrol_captures):\n\u001b[1;32m    245\u001b[0m     \u001b[39m# The caller must use record_operation to record this operation in the\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39m# eager case, so we enforce the same requirement for the non-eager\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[39m# case by explicitly pausing recording. We don't have a gradient\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[39m# registered for PartitionedCall, so recording this operation confuses\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39m# forwardprop code (GradientTape manages to ignore it).\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    252\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    253\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    254\u001b[0m             \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    255\u001b[0m             \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    256\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    143\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.10/site-packages/tensorflow/python/eager/record.py:64\u001b[0m, in \u001b[0;36mstop_recording\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_stopped:\n\u001b[0;32m---> 64\u001b[0m     pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeSetRestartOnThread()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if LEAVE_ONE_OUT:\n",
    "    leave_one_out()\n",
    "    exit()\n",
    "\n",
    "train_data, train_labels, dev_data, dev_labels, test_data, test_labels = create_data_sets(dev_size=DEV_SIZE, test_size=TEST_SIZE)\n",
    "\n",
    "# for printing \n",
    "RAW_TRAIN = train_data[0]\n",
    "\n",
    "if AUGMENT:\n",
    "    augmented_train_data, augmented_train_labels = augment_data(train_data, train_labels, AUGMENT_SIZE)\n",
    "    train_data = np.concatenate((train_data, augmented_train_data))\n",
    "    train_labels = np.concatenate((train_labels, augmented_train_labels))\n",
    "    print(\"length of training data with augmentation: \" + str(len(train_data)))\n",
    "\n",
    "if TRANSFORM:\n",
    "    train_transform, dev_transform, test_transform = extract_fft(train_data, dev_data, test_data)\n",
    "    train_data = np.array(train_data)\n",
    "    train_transform = np.array(train_transform)\n",
    "    dev_data = np.array(dev_data)\n",
    "    dev_transform = np.array(dev_transform)\n",
    "    test_data = np.array(test_data)\n",
    "    test_transform = np.array(test_transform)\n",
    "\n",
    "    train_data = np.array([merge_arrays(train_data[i], train_transform[i]) for i in range(len(train_data))])\n",
    "    dev_data = np.array([merge_arrays(dev_data[i], dev_transform[i]) for i in range(len(dev_data))])\n",
    "    test_data = np.array([merge_arrays(test_data[i], test_transform[i]) for i in range(len(test_data))])\n",
    "\n",
    "if DIFFERENTIALS:\n",
    "    train_differential, dev_differential, test_differential = extract_differentials(train_data, dev_data, test_data)\n",
    "    train_data = np.array(train_data)\n",
    "    train_differential = np.array(train_differential)\n",
    "    dev_data = np.array(dev_data)\n",
    "    dev_differential = np.array(dev_differential)\n",
    "    test_data = np.array(test_data)\n",
    "    test_differential = np.array(test_differential)\n",
    "\n",
    "    train_data = np.array([merge_arrays(train_data[i], train_differential[i]) for i in range(len(train_data))])\n",
    "    dev_data = np.array([merge_arrays(dev_data[i], dev_differential[i]) for i in range(len(dev_data))])\n",
    "    test_data = np.array([merge_arrays(test_data[i], test_differential[i]) for i in range(len(test_data))])\n",
    "\n",
    "if GRADIENTS:\n",
    "    train_derivatives, dev_derivatives, test_derivatives = extract_gradients(train_data, dev_data, test_data)\n",
    "    train_data = np.array(train_data)\n",
    "    train_derivatives = np.array(train_derivatives)\n",
    "    dev_data = np.array(dev_data)\n",
    "    dev_derivatives = np.array(dev_derivatives)\n",
    "    test_data = np.array(test_data)\n",
    "    test_derivatives = np.array(test_derivatives)\n",
    "\n",
    "    train_data = np.array([merge_arrays(train_data[i], train_derivatives[i]) for i in range(len(train_data))])\n",
    "    dev_data = np.array([merge_arrays(dev_data[i], dev_derivatives[i]) for i in range(len(dev_data))])\n",
    "    test_data = np.array([merge_arrays(test_data[i], test_derivatives[i]) for i in range(len(test_data))])\n",
    "\n",
    "\n",
    "EXTRACTED_TRAIN = train_data[0]\n",
    "\n",
    "# Write RAW_TRAIN to a text file\n",
    "with open('raw_train.txt', 'w') as file:\n",
    "    for row in RAW_TRAIN:\n",
    "        file.write('{' + ', '.join(str(value) for value in row) + '},\\n')\n",
    "\n",
    "# Write EXTRACTED_TRAIN to a text file\n",
    "with open('extracted_train.txt', 'w') as file:\n",
    "    for row in EXTRACTED_TRAIN:\n",
    "        file.write('{' + ', '.join(str(value) for value in row) + '},\\n')\n",
    "\n",
    "\n",
    "\n",
    "if CNN:\n",
    "    model = train_model_CNN(train_data, train_labels, POSSIBLE_OUTCOMES, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(dev_data, dev_labels)) \n",
    "else:\n",
    "    model = train_model_LSTM(train_data, train_labels, POSSIBLE_OUTCOMES, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(dev_data, dev_labels)) \n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "#test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "#print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "#model.save(f\"models/presentation_models/task2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
